'use client';
import React, { useEffect, useRef, useState } from 'react';

type EmotionResult = {
  avgVolume: number;     // å¹³å‡å“åº¦ 0..1
  pitchHz: number;       // ä¼°ç®—åŸºé¢‘
  speakingRate?: number; // å¯é€‰ï¼šæ¯ç§’å­—æ•°ï¼ˆç®€å•ä¼°è®¡ï¼‰
  label: 'calm' | 'neutral' | 'stressed';
};

export default function ReadAndAnalyze({
  text = 'Take a slow breath and read this aloud: â€œI am safe. I can slow down. My thoughts can pass like clouds.â€',
  seconds = 20,
  onDone,
}: {
  text?: string;
  seconds?: number;
  onDone?: (res: EmotionResult) => void;
}) {
  const [status, setStatus] = useState<'idle'|'recording'|'done'>('idle');
  const [left, setLeft] = useState(seconds);
  const [level, setLevel] = useState(0); // å®æ—¶èƒ½é‡
  const [pitch, setPitch] = useState(0);
  const volSum = useRef(0);
  const volCount = useRef(0);
  const pitchSum = useRef(0);
  const pitchCount = useRef(0);
  const wordsSpoken = useRef(0);
  const startedAt = useRef<number | null>(null);

  const audioCtx = useRef<AudioContext | null>(null);
  const src = useRef<MediaStreamAudioSourceNode | null>(null);
  const analyser = useRef<AnalyserNode | null>(null);
  const raf = useRef<number>();

  // ç®€æ˜“ pitch ä¼°è®¡ï¼ˆè‡ªç›¸å…³ï¼‰
  function estimatePitchAutoCorr(data: Float32Array, sampleRate: number) {
    let size = data.length;
    let maxShift = Math.floor(sampleRate / 50);  // æœ€ä½50Hz
    let minShift = Math.floor(sampleRate / 500); // æœ€é«˜500Hz
    let bestShift = -1;
    let bestCorr = 0;

    for (let shift = minShift; shift <= maxShift; shift++) {
      let corr = 0;
      for (let i = 0; i < size - shift; i++) {
        corr += data[i] * data[i + shift];
      }
      corr = corr / (size - shift);
      if (corr > bestCorr) {
        bestCorr = corr;
        bestShift = shift;
      }
    }

    if (bestShift > 0 && bestCorr > 0.02) {
      return sampleRate / bestShift;
    }
    return 0;
  }

  const tick = () => {
    const a = analyser.current!;
    const buf = new Float32Array(a.fftSize);
    a.getFloatTimeDomainData(buf);

    // èƒ½é‡ï¼ˆRMSï¼‰
    let sum = 0;
    for (let i = 0; i < buf.length; i++) sum += buf[i] * buf[i];
    const rms = Math.sqrt(sum / buf.length);
    const vol = Math.min(1, rms * 8); // æ”¾å¤§æ˜ å°„åˆ° 0..1
    setLevel(vol);
    volSum.current += vol;
    volCount.current++;

    // pitch
    const p = estimatePitchAutoCorr(buf, (audioCtx.current as AudioContext).sampleRate);
    setPitch(p);
    if (p > 50 && p < 500) {
      pitchSum.current += p;
      pitchCount.current++;
    }

    raf.current = requestAnimationFrame(tick);
  };

  async function start() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: true, noiseSuppression: true }});
      audioCtx.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      src.current = audioCtx.current.createMediaStreamSource(stream);
      analyser.current = audioCtx.current.createAnalyser();
      analyser.current.fftSize = 2048;
      src.current.connect(analyser.current);
      setStatus('recording');
      startedAt.current = performance.now();

      // å€’è®¡æ—¶
      setLeft(seconds);
      const timer = setInterval(() => {
        setLeft(l => {
          if (l <= 1) {
            clearInterval(timer);
            stop();
          }
          return l - 1;
        });
      }, 1000);

      raf.current = requestAnimationFrame(tick);
    } catch (e) {
      alert('Microphone permission denied.');
    }
  }

  function stop() {
    if (raf.current) cancelAnimationFrame(raf.current);
    if (audioCtx.current) {
      audioCtx.current.close();
      audioCtx.current = null;
    }
    setStatus('done');

    const avgVolume = volCount.current > 0 ? volSum.current / volCount.current : 0;
    const avgPitch = pitchCount.current > 0 ? pitchSum.current / pitchCount.current : 0;

    // æ–‡æœ¬æœ—è¯»é€Ÿç‡ï¼ˆç²—ç•¥ï¼‰ï¼šç»Ÿè®¡ç©ºæ ¼åˆ†å‰²çš„è¯æ•° / æ—¶é•¿
    let rate = 0;
    if (startedAt.current) {
      const secs = (performance.now() - startedAt.current) / 1000;
      const words = text.trim().split(/\s+/).length;
      rate = words / secs;
    }

    // ç®€å•æƒ…ç»ªè§„åˆ™ï¼ˆå¯æŒ‰éœ€è¦è°ƒå‚ï¼‰
    // éŸ³é‡åé«˜ + è¯­é€Ÿåå¿« -> stressed
    // éŸ³é‡ä¸­ç­‰/åä½ + pitch ç¨³å®š -> calm
    let label: EmotionResult['label'] = 'neutral';
    if (avgVolume > 0.35 && rate > 2.0) label = 'stressed';
    else if (avgVolume < 0.18 && rate < 1.6) label = 'calm';

    onDone?.({
      avgVolume: Number(avgVolume.toFixed(3)),
      pitchHz: Number(avgPitch.toFixed(1)),
      speakingRate: Number(rate.toFixed(2)),
      label,
    });
  }

  useEffect(() => {
    return () => {
      if (raf.current) cancelAnimationFrame(raf.current);
      if (audioCtx.current) audioCtx.current.close();
    };
  }, []);

  return (
    <div className="panel glass">
      <h3 style={{ marginTop: 0 }}>Step 6 Â· Read Aloud & Emotion Check</h3>
      <p className="muted" style={{ marginTop: 2 }}>
        Read the sentence slowly and gently. Keep a steady rhythm and soft volume.
      </p>

      <div style={{ padding: '10px 12px', borderRadius: 12, background: 'rgba(255,255,255,.04)', border: '1px solid rgba(255,255,255,.08)', marginTop: 8 }}>
        <em style={{ opacity: .9 }}>{text}</em>
      </div>

      <div style={{ display: 'flex', gap: 12, alignItems: 'center', marginTop: 12 }}>
        <button className="btn" onClick={() => (status === 'recording' ? stop() : start())}>
          {status === 'recording' ? 'Stop' : 'Start'}
        </button>
        {status === 'recording' && <span className="badge">â± {left}s</span>}
        <span className="badge">ğŸš volume {(level*100)|0}%</span>
        <span className="badge">ğŸµ pitch {pitch ? `${pitch.toFixed(0)} Hz` : '--'}</span>
      </div>
    </div>
  );
}
